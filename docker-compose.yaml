services:
  # Airflow Initialization Service
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow # Use the dedicated Airflow Dockerfile
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__CORE__FERNET_KEY: w8sS3Qp6mhQjt5ePpHQvwsdVdrZdj4a7QIgBvPo4VFk= # IMPORTANT: Replace with a real Fernet key
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
      AIRFLOW__CORE__DEFAULT_TIMEZONE: "utc"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./scripts:/opt/airflow/scripts
      - ./datamart:/opt/airflow/datamart
      - ./models:/opt/airflow/models
      - ./mlflow:/mlflow
      - ./mlruns:/mlflow/mlruns
      - airflow_data:/opt/airflow
    entrypoint: >
      /bin/bash -c "
      airflow db init &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true"

# Airflow Webserver Service
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow # Use the dedicated Airflow Dockerfile
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__CORE__FERNET_KEY: w8sS3Qp6mhQjt5ePpHQvwsdVdrZdj4a7QIgBvPo4VFk= # IMPORTANT: Replace with a real Fernet key
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
      AIRFLOW__CORE__DEFAULT_TIMEZONE: "utc"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./scripts:/opt/airflow/scripts
      - ./datamart:/opt/airflow/datamart
      - ./models:/opt/airflow/models
      - ./mlflow:/mlflow
      - ./mlruns:/mlflow/mlruns
      - airflow_data:/opt/airflow
    command: ["bash", "-c", "echo '---------------------------------------------------' && echo 'Services are starting up...' && echo 'Airflow UI: http://localhost:8080' && echo 'MLflow UI:  http://localhost:5000' && echo 'Jupyter Lab: http://localhost:8888' && echo '---------------------------------------------------' && exec airflow webserver"]
    ports:
      - "8080:8080"
    depends_on:
      - airflow-init # Webserver needs the database initialized

  # Airflow Scheduler Service
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow # Use the dedicated Airflow Dockerfile
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__CORE__FERNET_KEY: w8sS3Qp6mhQjt5ePpHQvwsdVdrZdj4a7QIgBvPo4VFk= # IMPORTANT: Replace with a real Fernet key
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
      AIRFLOW__CORE__DEFAULT_TIMEZONE: "utc"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./scripts:/opt/airflow/scripts
      - ./datamart:/opt/airflow/datamart
      - ./models:/opt/airflow/models
      - ./mlflow:/mlflow
      - ./mlruns:/mlflow/mlruns
      - airflow_data:/opt/airflow
    command: scheduler
    depends_on:
      - airflow-webserver # Scheduler typically relies on webserver for UI/health checks

  # Jupyter Lab Service
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile.jupyter # Use the dedicated Jupyter Dockerfile
    container_name: jupyter_lab
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - MLFLOW_TRACKING_URI=http://mlflow:5000 # Points to the MLflow service
    volumes:
      - .:/app # Mount current directory into /app for Jupyter notebooks
      - ./mlruns:/app/mlruns # Mount MLflow artifacts directory
    depends_on:
      - mlflow # Jupyter needs MLflow tracking server to be available

  # MLflow Tracking Server Servicemlflow:
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.11.3
    container_name: mlflow_tracking
    ports:
      - "5000:5000"
    environment:
      MLFLOW_TRACKING_URI: http://0.0.0.0:5000
      BACKEND_STORE_URI: sqlite:///mlflow/mlflow.db
      ARTIFACT_ROOT: /mlflow/mlruns
    command: >
      mlflow server
      --backend-store-uri sqlite:///mlflow/mlflow.db
      --default-artifact-root /mlflow/mlruns
      --host 0.0.0.0
      --port 5000
    volumes:
      - ./mlruns:/mlflow/mlruns
      - ./mlflow:/mlflow
      - ./models:/opt/airflow/models

  streamlit:
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    ports:
      - "8501:8501"
    volumes:
      - ./scripts:/app
      - ./datamart:/app/datamart
    depends_on:
      - mlflow

volumes:
  airflow_data: # Named volume for Airflow's internal data
